{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from openimages import OpenImageDataset\n",
    "from openimages import OpenImagesConfig\n",
    "import openimages as oim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(OpenImagesConfig):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"openimages\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model in inference mode\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                              config=inference_config,\n",
    "                              model_dir=MODEL_DIR)\n",
    "  \n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "#model_path = model.find_last()\n",
    "model_path = os.path.join(ROOT_DIR, 'logs/openimages20190926T0931/mask_rcnn_openimages_0018.h5')\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.path.join(ROOT_DIR, '../data/'))\n",
    "\n",
    "# Validation dataset\n",
    "dataset = OpenImageDataset()\n",
    "dataset.load_dataset(PATH, 'kaggle-test')\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = [\"ImageID,ImageWidth,ImageHeight,PredictionString\"]\n",
    "\n",
    "for image_id in tqdm(dataset.image_ids):\n",
    "\n",
    "    original_image = dataset.load_image(image_id)\n",
    "    \n",
    "    results = model.detect([original_image], verbose=0)\n",
    "\n",
    "    r = results[0]\n",
    "    n_detections = r['masks'].shape[2]\n",
    "    \n",
    "    detections = []\n",
    "    source_image = dataset.image_info[image_id]['id']\n",
    "    image_size = dataset.image_info[image_id]['size']\n",
    "    \n",
    "    for detection_index in range(n_detections):\n",
    "\n",
    "        mask = r['masks'][:, :, detection_index]\n",
    "        score = r['scores'][detection_index]\n",
    "        class_id = r['class_ids'][detection_index]\n",
    "        bbox = r['rois'][detection_index, :]\n",
    "\n",
    "        label = dataset.get_source_class_id(class_id, 'openimages')\n",
    "\n",
    "        \n",
    "        #true_mask = oim.unmold_mask(mask, bbox, image_size)\n",
    "        true_mask = mask\n",
    "        \n",
    "        rle_mask = oim.encode_binary_mask(true_mask)\n",
    "       \n",
    "        detections.extend([label, str(score), rle_mask.decode('ascii')])\n",
    "        \n",
    "    detections_str = \" \".join(detections)\n",
    "    row = \",\".join([source_image, str(image_size[0]), str(image_size[1]), detections_str])\n",
    "    all_rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete with empty detections if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_rows) < len(dataset.image_ids) + 1:\n",
    "    for image_info in dataset.image_info[len(all_rows)-1:]:\n",
    "        all_rows.append(\",\".join([image_info['id'], '-1', '-1', '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as fid:\n",
    "    fid.write(\"\\n\".join(all_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
